{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import statsmodels as sm\n",
    "from tqdm import tqdm\n",
    "import urllib\n",
    "import urllib2, requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------    \n",
    "#Scraping lat & lng from each apartment:\n",
    "\n",
    "#Open the refer csv file and create a list with the reference number:\n",
    "df = pd.read_csv(\"../datasets/refer\")\n",
    "lista = list(df[\"0\"])\n",
    "lista1 = lista[0:89601]\n",
    "#Create a list with the urls that we will extract the data:\n",
    "\n",
    "url = []\n",
    "for i in lista1:\n",
    "    url.append(\"http://www.squarefoot.com.hk/property/\" + str(i))\n",
    "    \n",
    "#Dictionary with the url and the ref number:\n",
    "lel = dict(zip(url, lista1))\n",
    "\n",
    "#Iterate in the dictionary and save the html files in our neverend folder:\n",
    "for i, j in tqdm(lel.iteritems()):\n",
    "    try:\n",
    "        html_content = urllib2.urlopen(i)\n",
    "        content = html_content.read()\n",
    "        folder = \"neverend\"\n",
    "        filename = folder + '/' + str(j) + \".html\" \n",
    "        with file(filename, \"wt\") as fo:\n",
    "            fo.write(content)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "#Checking that there are no offers repeated for the same apartment:\n",
    "ue = list(df[\"0\"].unique())\n",
    "\n",
    "#List with the address of our html files:\n",
    "refdoc = []\n",
    "for i in ue:\n",
    "    x = \"file:///C:/Users/alexrebull/.babun/cygwin/home/alexrebull/DSI-HK-1/capstone/Assets/scraping/neverend/\" + str(i) + \".html\"\n",
    "    refdoc.append(x)\n",
    "    \n",
    "#Extract the latitude and longitude for each one of our html files:\n",
    "lat = []\n",
    "lng = []\n",
    "for i in tqdm(refdoc):\n",
    "    try:\n",
    "        word = \"LatLng((.*))\"\n",
    "        contents = BeautifulSoup(urllib.urlopen(i), \"lxml\")  \n",
    "        contentsElements = contents.find_all('script', attrs={'type' : 'text/javascript'})\n",
    "        for t in contentsElements:\n",
    "            for u in t:\n",
    "                for p in t:\n",
    "                    if \"LatLng\" in p:\n",
    "                        print p\n",
    "                        le = \"\".join(p)\n",
    "                        pos = re.search(word, le)\n",
    "                        coord = pos.group(1)\n",
    "                        lat.append(coord.split(\",\")[0])\n",
    "                        lng.append(coord.split(\",\")[1])\n",
    "                        \n",
    "    except:\n",
    "        lat.append(None)\n",
    "        lng.append(None) \n",
    "        \n",
    "#Cleaning the lat and lng features:\n",
    "lat1 = []\n",
    "for i in lat:\n",
    "    if i != None:\n",
    "        lat1.append(i.split(\"(\")[1])\n",
    "    else:\n",
    "        lat1.append(None)\n",
    "        \n",
    "lng1 = []\n",
    "for i in lng:\n",
    "    if i != None:\n",
    "        lng1.append(i.split(\")\")[0])\n",
    "    else:\n",
    "        lng1.append(None)\n",
    "        \n",
    "#Create dictionaries for lat and lng, merge both dataframes and save it as a csv file called \"coordapart\":\n",
    "dictlat = dict(zip(ue, lat1))\n",
    "dictlng = dict(zip(ue, lng1))\n",
    "\n",
    "dflat = pd.DataFrame(dictlat.items(), columns=['ref', 'lat'])\n",
    "dlng = pd.DataFrame(dictlng.items(), columns=[\"ref\", \"lng\"])\n",
    "\n",
    "result = pd.merge(dflat, dlng, how='inner', on=[\"ref\"])\n",
    "\n",
    "result.to_csv(\"datasets/coordapart\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------    \n",
    "#Scraping the data from each apartment:\n",
    "layout = []\n",
    "floor = []\n",
    "area = []\n",
    "age = []\n",
    "address = []\n",
    "manfee = []\n",
    "rate = []\n",
    "out_area = []\n",
    "bath = []\n",
    "attr = []\n",
    "ref = []\n",
    "for i in tqdm(refdoc):\n",
    "    form = []\n",
    "    try:\n",
    "        contents = BeautifulSoup(urllib2.urlopen(i), \"lxml\")\n",
    "        contentsElements = contents.findAll('table', attrs={'id':'DetailTable'})\n",
    "        for i in contentsElements:\n",
    "            xp = i.findAll(\"td\", attrs={\"class\" : \"output\"})\n",
    "            for t in xp:\n",
    "                form.append(t.getText())\n",
    "            if len(form) == 15:\n",
    "                layout.append(form[1])\n",
    "                floor.append(form[2])\n",
    "                area.append(form[4])\n",
    "                address.append(form[6])\n",
    "                age.append(form[7])\n",
    "                manfee.append(form[10])\n",
    "                rate.append(form[11])\n",
    "                out_area.append(form[12])\n",
    "                bath.append(form[14])\n",
    "            else:\n",
    "                layout.append(form[1])\n",
    "                floor.append(form[2])\n",
    "                area.append(form[4])\n",
    "                address.append(form[8])\n",
    "                age.append(form[9])\n",
    "                manfee.append(form[12])\n",
    "                rate.append(form[13])\n",
    "                out_area.append(form[14])\n",
    "                bath.append(form[16])\n",
    "                \n",
    "        contentsElements1 = contents.findAll('table', attrs={'border':'0'})\n",
    "        for p in contentsElements1:\n",
    "            attr.append(\"\".join(p.getText()))\n",
    "        contentsElements2 = contents.findAll('div', attrs={'class':'propertyTitle'})\n",
    "        for pl in contentsElements2:\n",
    "            ref.append(pl.getText().split()[-1])\n",
    "    except:\n",
    "        next\n",
    "       \n",
    "    \n",
    "#Cleanining some features:\n",
    "floor1 = []\n",
    "area1 = []\n",
    "address1 = []\n",
    "age1 = []\n",
    "bath1= []\n",
    "\n",
    "for i in floor:\n",
    "    floor1.append(i.split(\"\\r\\n\")[1].strip())\n",
    "for i in area:\n",
    "    area1.append(i.split(\"\\r\\n\")[1].strip()) \n",
    "for i in address:\n",
    "    address1.append(i.split(\"\\r\\n\")[1].strip())  \n",
    "for i in age:\n",
    "    age1.append(i.split(\"\\r\\n\")[1].strip())\n",
    "for i in bath:\n",
    "    bath1.append(i.split(\"\\r\\n\")[1].strip())\n",
    "\n",
    "#Create a dataframe and save it as a csv file:\n",
    "df = pd.DataFrame({\"ref\" : ref, \"floor\": floor1, \"area\" : area, \"age\" : age1, \"bath\" : bath1, \"attr\" = attr})\n",
    "\n",
    "df.to_csv(\"datasets/smalldf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
